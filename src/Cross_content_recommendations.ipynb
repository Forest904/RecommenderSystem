{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, here's several helpful packages to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful libraries\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing datasets\n",
    "#Books\n",
    "books_file = 'datasets/books_rs/books.csv'\n",
    "df_books = pd.read_csv(books_file)\n",
    "\n",
    "#Movies\n",
    "movies_file = 'datasets/books_rs/movies.csv'\n",
    "df_movies = pd.read_csv(movies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Preprocessing. \n",
    "Let's make the two datasets omogeneus.  First reordering.\n",
    "Now we separate the ratings from the actual content, before removing the unnecessary attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column to identify the type of content\n",
    "df_books['Type'] = 'book' \n",
    "df_movies['Type'] = 'movie'\n",
    "\n",
    "# Separate ratings and content dataframes\n",
    "rating_cols = ['Title', 'Vote Count', 'Vote Average']\n",
    "df_books_ratings = df_books[rating_cols]\n",
    "df_movies_ratings = df_movies[rating_cols]\n",
    "df_books_content = df_books.drop(['Vote Count', 'Vote Average'], axis=1)\n",
    "df_movies_content = df_movies.drop(['Vote Count', 'Vote Average'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Combining Datasets. \n",
    "Merge the books and movies datasets into single DataFrames for content and ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_content = pd.concat([df_books_content, df_movies_content], ignore_index=True)\n",
    "cross_rating = pd.concat([df_books_ratings, df_movies_ratings], ignore_index=True)\n",
    "\n",
    "title_type_df = cross_content[['Title', 'Type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Processing Categories. \n",
    "Convert the 'categories' string into a list of categories for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'Genres' into 'categories_list'\n",
    "def process_categories(categories_str):\n",
    "    if pd.isnull(categories_str):\n",
    "        return []\n",
    "    try:\n",
    "        # Try to parse the string as a list\n",
    "        categories = ast.literal_eval(categories_str)\n",
    "        if isinstance(categories, list):\n",
    "            return [str(cat).strip().lower() for cat in categories]\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    # Split by common delimiters if not a list\n",
    "    return [cat.strip().lower() for cat in re.split(r'[;,]', categories_str)]\n",
    "\n",
    "cross_content['categories_list'] = cross_content['Genres'].apply(process_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Text Preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Now we condense all the non title attributes in one TAGS column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Now we apply preprocessing to clean the tags column to remove all non essential caracteristics to clean and standardize text data for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words and stemmer once\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "word_pattern = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and extract words\n",
    "    words = word_pattern.findall(text.lower())\n",
    "    # Remove stop words and stem\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Combining 'tags' and 'categories_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 'tags' and 'categories_list'\n",
    "cross_content['tags'] = cross_content[['Author', 'Plot', 'Type']].fillna('').agg(' '.join, axis=1)\n",
    "cross_content['tags'] = cross_content['tags'].apply(preprocess_text)\n",
    "cross_content['categories_list'] = cross_content['categories_list'].apply(\n",
    "    lambda x: ' '.join(preprocess_text(' '.join(x)))\n",
    ")\n",
    "\n",
    "# Combine 'tags' and 'categories_list' into 'combined_tags'\n",
    "cross_content['combined_tags'] = cross_content['tags'] + ' ' + cross_content['categories_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Before splitting the data, we need to ensure that each item in cross_content has its corresponding average_rating for prediction and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'average_rating' into 'cross_content' for prediction\n",
    "cross_content = cross_content.merge(cross_rating[['Title', 'Vote Average']], on='Title', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the cross_content dataframe is clean with a title key and a tags string rappresentative of the content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Splitting Data.\n",
    "Divide the data into training and testing sets to evaluate the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_cross_train, X_cross_test = train_test_split(cross_content, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. TF-IDF vectorization.\n",
    "Convert textual data into numerical vectors using TF-IDF for similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF matrix on 'combined_tags'\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "tfidf_matrix = tf.fit_transform(cross_content['combined_tags'])\n",
    "\n",
    "# Get TF-IDF vectors for training and test sets\n",
    "train_tfidf_matrix = tfidf_matrix[X_cross_train.index]\n",
    "test_tfidf_matrix = tfidf_matrix[X_cross_test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Computing Cosine Similarities.\n",
    "Calculate the similarity between each test item and all training items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_test_train = cosine_similarity(test_tfidf_matrix, train_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Evaluation Metrics.\n",
    "\n",
    "Evaluate the performance of the recommender system using standard metrics.\n",
    "    Initialize (TP), (FP), (FN), (TN).\n",
    "    Loop through each test item:\n",
    "    Retrieve the test item's categories, skipping items without categories.\n",
    "    Compute similarity scores to all training items.\n",
    "    Select top k recommended items.\n",
    "    For each recommended item, check if it shares categories with the test item.\n",
    "    Update TP and FP counts accordingly.\n",
    "    Calculate FN and TN based on the total number of relevant and non-relevant items.\n",
    "    After processing all test items, compute Precision, Recall, Accuracy, and F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ratings from the training set\n",
    "train_ratings = X_cross_train['Vote Average'].values\n",
    "\n",
    "k = 10  # Number of similar contents to consider\n",
    "\n",
    "# Get indices of top k similar train items for each test item\n",
    "top_k_indices = np.argsort(-cosine_sim_test_train, axis=1)[:, :k]\n",
    "\n",
    "# Get top k similarities and ratings\n",
    "top_k_similarities = np.take_along_axis(cosine_sim_test_train, top_k_indices, axis=1)\n",
    "top_k_ratings = train_ratings[top_k_indices]\n",
    "\n",
    "# Compute predicted ratings\n",
    "numerators = np.sum(top_k_similarities * top_k_ratings, axis=1)\n",
    "denominators = np.sum(top_k_similarities, axis=1)\n",
    "\n",
    "# Handle zero denominators\n",
    "predicted_ratings = np.divide(numerators, denominators, out=np.zeros_like(numerators), where=denominators != 0)\n",
    "# Replace zero denominators with mean of top_k_ratings\n",
    "mean_top_k_ratings = np.mean(top_k_ratings, axis=1)\n",
    "predicted_ratings = np.where(denominators != 0, predicted_ratings, mean_top_k_ratings)\n",
    "\n",
    "# Get actual ratings\n",
    "actual_ratings = X_cross_test['Vote Average'].values\n",
    "\n",
    "# Exclude items where actual rating is NaN\n",
    "valid_indices = ~np.isnan(actual_ratings)\n",
    "actual_ratings = actual_ratings[valid_indices]\n",
    "predicted_ratings = predicted_ratings[valid_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Computing metrics.\n",
    "After processing all test items, we compute the overall metrics.\n",
    "Precision: Proportion of recommended items that are relevant.\n",
    "Recall: Proportion of relevant items that are recommended.\n",
    "F1 Score: Harmonic mean of Precision and Recall.\n",
    "Mean Squared Error (MSE): Measures the average squared difference between actual and predicted ratings.\n",
    "Root Mean Squared Error (RMSE): Square root of MSE, providing error in the same units as the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics\n",
    "threshold = 5\n",
    "actual_binary = (actual_ratings > threshold).astype(int)\n",
    "predicted_binary = (predicted_ratings > threshold).astype(int)\n",
    "\n",
    "precision = precision_score(actual_binary, predicted_binary)\n",
    "recall = recall_score(actual_binary, predicted_binary)\n",
    "f1 = f1_score(actual_binary, predicted_binary)\n",
    "accuracy = accuracy_score(actual_binary, predicted_binary)\n",
    "\n",
    "mse = mean_squared_error(actual_ratings, predicted_ratings)\n",
    "rmse = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Recommendation Function.\n",
    "\n",
    "Try to find the index of the item with the specified title.\n",
    "Compute cosine similarity between the item's TF-IDF vector and all items.\n",
    "Sort the similarity scores in descending order.\n",
    "Exclude the item itself from recommendations.\n",
    "Retrieve the titles of the top k recommended items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get recommendations for a content title\n",
    "def get_recommendations(content_title, k):\n",
    "    # Get the index of the content with the given title\n",
    "    try:\n",
    "        content_index = cross_content[cross_content['Title'] == content_title].index[0]\n",
    "    except IndexError:\n",
    "        print(f\"Content titled '{content_title}' not found.\")\n",
    "        return []\n",
    "\n",
    "    # Get the similarity scores for the content_index\n",
    "    # Compute cosine similarity between the content and all contents\n",
    "    sim_scores = cosine_similarity(tfidf_matrix[content_index], tfidf_matrix).flatten()\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    sim_scores_indices = sim_scores.argsort()[::-1]\n",
    "\n",
    "    # Get the indices of the top k+1 similar contents (excluding the content itself)\n",
    "    top_indices = [i for i in sim_scores_indices if i != content_index][:k]\n",
    "\n",
    "    # Get the titles of the recommended contents\n",
    "    recommended_titles = cross_content.iloc[top_indices]['Title'].tolist()\n",
    "\n",
    "    return recommended_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Printing Recommendations.\n",
    "Generate and display recommendations for a specific content title. Sort the recommendations based on 'average_rating' and 'ratings_count' for better results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_content_type(titles):\n",
    "    for title in titles:\n",
    "        content_type_row = title_type_df[title_type_df['Title'] == title]\n",
    "        if not content_type_row.empty:\n",
    "            content_type = content_type_row['Type'].values[0]\n",
    "            if content_type == 'book':\n",
    "                print(f\"{title} (Book)\")\n",
    "            elif content_type == 'movie':\n",
    "                print(f\"{title} (Movie)\")\n",
    "            else:\n",
    "                print(f\"{title} (Unknown Content Type)\")\n",
    "        else:\n",
    "            print(f\"{title} (Content Type Not Found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Actually running the algorithm with a title with an example usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_title = \"The Avengers\"\n",
    "recommendations = get_recommendations(content_title, k)\n",
    "\n",
    "# Sort recommendations by 'Vote Average' and 'Vote Count'\n",
    "sorted_recommendations = cross_rating[cross_rating['Title'].isin(recommendations)] \\\n",
    "    .sort_values(['Vote Average', 'Vote Count'], ascending=[False, False])['Title'].tolist()\n",
    "\n",
    "print_content_type(sorted_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Printing validation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valuation of the recommender system\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'f1', 'recall', 'mse', 'rmse']\n",
    "values = [accuracy, precision, f1, recall, mse, rmse]\n",
    "\n",
    "plt.bar(metrics, values)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Metrics Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
