{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, here's several helpful packages to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing datasets\n",
    "#Books\n",
    "books_file = 'datasets/books_rs/books.csv'\n",
    "df_books = pd.read_csv(books_file)\n",
    "\n",
    "#Movies\n",
    "movies_file = 'datasets/books_rs/movies.csv'\n",
    "df_movies = pd.read_csv(movies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the two datasets omogeneus.  First reordering.\n",
    "Now we separate the ratings from the actual content, before removing the unnecessary attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reindexing columns\n",
    "df_books = df_books.reindex(columns=['title', 'categories', 'authors', 'description', 'ratings_count', 'average_rating', 'published_year', 'subtitle', 'isbn13'])\n",
    "df_movies = df_movies.reindex(columns=['Series_Title', 'Genre', 'Director', 'Overview', 'No_of_Votes', 'IMDB_Rating', 'Released_Year'])\n",
    "\n",
    "#Merge title and subtitle columns for books\n",
    "df_books['title'] = df_books['title'] + df_books['subtitle'].apply(lambda x: ' : ' + x if pd.notnull(x) else '')\n",
    "\n",
    "#Drop unused columns\n",
    "df_books = df_books.drop(['published_year', 'subtitle', 'isbn13'], axis=1, errors='ignore')\n",
    "df_movies = df_movies.drop(['Released_Year'], axis=1, errors='ignore')\n",
    "\n",
    "#Rename the columns to be the same as the books\n",
    "df_movies.columns = ['title', 'categories', 'authors', 'description', 'ratings_count', 'average_rating']\n",
    "\n",
    "#Add a column to identify the type of content\n",
    "df_books['content_type'] = 'book' \n",
    "df_movies['content_type'] = 'movie'\n",
    "\n",
    "#Save separated ratings and content dataframes for books and movies\n",
    "df_books_ratings = df_books[['title', 'ratings_count', 'average_rating']]\n",
    "df_books_content = df_books.drop(['ratings_count', 'average_rating'], axis=1, errors='ignore')\n",
    "\n",
    "df_movies_ratings = df_movies[['title', 'ratings_count', 'average_rating']]\n",
    "df_movies_content = df_movies.drop(['ratings_count', 'average_rating'], axis=1, errors='ignore')\n",
    "\n",
    "#Normalize the books ratings\n",
    "df_books_ratings.loc[:, 'average_rating'] = df_books_ratings['average_rating'] * 2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the two content dataframes, keeping an eye on the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_books_content dimensions: (1000, 5)\n",
      "df_movies_content dimensions: (1000, 5)\n",
      "cross_content dimensions: (2000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_books_content dimensions:\", df_books_content.shape)\n",
    "print(\"df_movies_content dimensions:\", df_movies_content.shape)\n",
    "cross_content = pd.concat([df_books_content, df_movies_content])\n",
    "print(\"cross_content dimensions:\", cross_content.shape)\n",
    "#And do the same fot the ratings\n",
    "cross_rating = pd.concat([df_books_ratings, df_movies_ratings])\n",
    "\n",
    "\n",
    "title_type_df = cross_content[['title', 'content_type']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we condense all the non title attributes in one TAGS column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_content['tags'] = cross_content.apply(lambda row: '; '.join([str(row[col]) for col in cross_content.columns if col != 'title']), axis=1)\n",
    "cross_content = cross_content.drop(['categories', 'authors', 'description', 'content_type'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply preprocessing to clean the tags column to remove all non essential caracteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stop words if running for the first time!!!\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 3. Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # 4. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back to a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "cross_content['tags'] = cross_content['tags'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the cross_content dataframe is clean with a title key and a tags string rappresentative of the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.0)\n",
    "tfidf_matrix = tf.fit_transform(cross_content['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use cosine similarity to compare the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wizard's First Rule (Book)\n",
      "The Pillars of Creation (Book)\n",
      "Naked Empire (Book)\n",
      "Debt of Bones (Book)\n",
      "Chainfire (Book)\n",
      "The Light Fantastic (Book)\n",
      "Ilse Witch (Book)\n",
      "Magic Kingdom for Sale : Sold (Book)\n",
      "Magician : Master (Book)\n",
      "Forever Odd (Book)\n",
      "Monsters, Inc. (Movie)\n",
      "Home Alone (Movie)\n",
      "The Satanic Verses : A Novel (Book)\n",
      "The Naked and the Dead : 50th Anniversary Edition, With a New Introduction by the Author (Book)\n",
      "Terminator 2: Judgment Day (Movie)\n",
      "Idi i smotri (Movie)\n",
      "Stone of Farewell (Book)\n",
      "A Clash of Kings (Book)\n",
      "Job, a Comedy of Justice (Book)\n",
      "Ballet Shoes (Book)\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "def get_recommendations(content_title, k):\n",
    "    # Get the index of the content with the given title\n",
    "    content_index = cross_content[cross_content['title'] == content_title].index[0]\n",
    "\n",
    "    # Get the similarity scores for the content_index\n",
    "    sim_scores = list(enumerate(cosine_sim[content_index]))\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the indices of the top k+1 similar contents (including the content itself)\n",
    "    top_indices = [i[0] for i in sim_scores[1:k+1]]\n",
    "\n",
    "    # Get the titles of the recommended contents\n",
    "    recommended_titles = cross_content.iloc[top_indices]['title'].tolist()\n",
    "\n",
    "    return recommended_titles\n",
    "\n",
    "k = 8  # Number of similar contents to recommend\n",
    "content_title = \"The Avengers\"\n",
    "recommendations = get_recommendations(content_title, 20)\n",
    "\n",
    "def print_content_type(titles):\n",
    "    for title in titles:\n",
    "        content_type = title_type_df.loc[title_type_df['title'] == title, 'content_type'].values[0]\n",
    "        if content_type == 'book':\n",
    "            print(f\"{title} (Book)\")\n",
    "        elif content_type == 'movie':\n",
    "            print(f\"{title} (Movie)\")\n",
    "        else:\n",
    "            print(f\"{title} (Unknown Content Type)\")\n",
    "\n",
    "# Example usage:\n",
    "print_content_type(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen the titles recommended are not too relevant because average ratings and the number of ratings are not taken into consideration yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step saving fuck all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_content.to_csv('cross_content.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
