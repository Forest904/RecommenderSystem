{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, here's several helpful packages to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing datasets\n",
    "#Books\n",
    "books_file = 'datasets/books_rs/books.csv'\n",
    "df_books = pd.read_csv(books_file)\n",
    "\n",
    "#Movies\n",
    "movies_file = 'datasets/books_rs/movies.csv'\n",
    "df_movies = pd.read_csv(movies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Preprocessing. \n",
    "Let's make the two datasets omogeneus.  First reordering.\n",
    "Now we separate the ratings from the actual content, before removing the unnecessary attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reindexing columns\n",
    "df_books = df_books.reindex(columns=['title', 'categories', 'authors', 'description', 'ratings_count', 'average_rating', 'published_year', 'subtitle', 'isbn13'])\n",
    "df_movies = df_movies.reindex(columns=['Series_Title', 'Genre', 'Director', 'Overview', 'No_of_Votes', 'IMDB_Rating', 'Released_Year'])\n",
    "\n",
    "#Merge title and subtitle columns for books\n",
    "df_books['title'] = df_books['title'] + df_books['subtitle'].apply(lambda x: ' : ' + x if pd.notnull(x) else '')\n",
    "\n",
    "#Drop unused columns\n",
    "df_books = df_books.drop(['published_year', 'subtitle', 'isbn13'], axis=1, errors='ignore')\n",
    "df_movies = df_movies.drop(['Released_Year'], axis=1, errors='ignore')\n",
    "\n",
    "#Rename the columns to be the same as the books\n",
    "df_movies.columns = ['title', 'categories', 'authors', 'description', 'ratings_count', 'average_rating']\n",
    "\n",
    "#Add a column to identify the type of content\n",
    "df_books['content_type'] = 'book' \n",
    "df_movies['content_type'] = 'movie'\n",
    "\n",
    "#Save separated ratings and content dataframes for books and movies\n",
    "df_books_ratings = df_books[['title', 'ratings_count', 'average_rating']]\n",
    "df_books_content = df_books.drop(['ratings_count', 'average_rating'], axis=1, errors='ignore')\n",
    "\n",
    "df_movies_ratings = df_movies[['title', 'ratings_count', 'average_rating']]\n",
    "df_movies_content = df_movies.drop(['ratings_count', 'average_rating'], axis=1, errors='ignore')\n",
    "\n",
    "#Normalize the books ratings\n",
    "df_books_ratings.loc[:, 'average_rating'] = df_books_ratings['average_rating'] * 2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Combining Datasets. \n",
    "Merge the books and movies datasets into single DataFrames for content and ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"df_books_content dimensions:\", df_books_content.shape)\n",
    "#print(\"df_movies_content dimensions:\", df_movies_content.shape)\n",
    "cross_content = pd.concat([df_books_content, df_movies_content], ignore_index=True)\n",
    "#print(\"cross_content dimensions:\", cross_content.shape)\n",
    "#And do the same fot the ratings\n",
    "cross_rating = pd.concat([df_books_ratings, df_movies_ratings], ignore_index=True)\n",
    "\n",
    "title_type_df = cross_content[['title', 'content_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Processing Categories. \n",
    "Convert the 'categories' string into a list of categories for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 'categories' field into a list before creating 'tags'\n",
    "def process_categories(categories_str):\n",
    "    if pd.isnull(categories_str):\n",
    "        return []\n",
    "    elif isinstance(categories_str, list):\n",
    "        # Flatten the list\n",
    "        categories = []\n",
    "        for item in categories_str:\n",
    "            if isinstance(item, list):\n",
    "                categories.extend(item)\n",
    "            else:\n",
    "                categories.append(str(item))\n",
    "        return [cat.strip().lower() for cat in categories]\n",
    "    else:\n",
    "        # If the string looks like a list, convert it\n",
    "        categories_str = str(categories_str)\n",
    "        if categories_str.startswith('[') and categories_str.endswith(']'):\n",
    "            import ast\n",
    "            try:\n",
    "                categories = ast.literal_eval(categories_str)\n",
    "                return [str(cat).strip().lower() for cat in categories]\n",
    "            except (ValueError, SyntaxError):\n",
    "                pass\n",
    "        # Split by comma or semicolon\n",
    "        return [cat.strip().lower() for cat in re.split(',|;', categories_str)]\n",
    "\n",
    "cross_content['categories_list'] = cross_content['categories'].apply(process_categories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Text Preprocessing. \n",
    "a. Ensure 'categories_list' is a flat list of strings without nested lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten any nested lists in 'categories_list'\n",
    "def flatten_categories(categories):\n",
    "    flat_list = []\n",
    "    for item in categories:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(flatten_categories(item))\n",
    "        else:\n",
    "            flat_list.append(str(item))\n",
    "    return flat_list\n",
    "\n",
    "cross_content['categories_list'] = cross_content['categories_list'].apply(flatten_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Now we condense all the non title attributes in one TAGS column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create 'tags' column without dropping 'categories_list'\n",
    "cross_content['tags'] = cross_content.apply(lambda row: '; '.join([str(row[col]) for col in ['authors', 'description', 'content_type'] if col in cross_content.columns and pd.notnull(row[col])]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Now we apply preprocessing to clean the tags column to remove all non essential caracteristics to clean and standardize text data for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stop words if running for the first time!!!\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 3. Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # 4. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back to a single string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Combining 'tags' and 'categories_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 'tags' and 'categories_list'\n",
    "cross_content['tags'] = cross_content['tags'].apply(preprocess_text)\n",
    "cross_content['categories_list'] = cross_content['categories_list'].apply(lambda x: [preprocess_text(cat) for cat in x])\n",
    "\n",
    "# Combine 'tags' and 'categories_list' into a single 'combined_tags' column\n",
    "cross_content['combined_tags'] = cross_content.apply(lambda row: ' '.join(row['tags'].split() + row['categories_list']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the cross_content dataframe is clean with a title key and a tags string rappresentative of the content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Splitting Data.\n",
    "Divide the data into training and testing sets to evaluate the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for training and testing\n",
    "X_cross_train, X_cross_test = train_test_split(cross_content, test_size=0.2, random_state=42)\n",
    "train_indices = X_cross_train.index\n",
    "test_indices = X_cross_test.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. TF-IDF vectorization.\n",
    "Convert textual data into numerical vectors using TF-IDF for similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.0)\n",
    "tfidf_matrix = tf.fit_transform(cross_content['tags'])\n",
    "\n",
    "# Get TF-IDF vectors for training and test sets\n",
    "train_tfidf_matrix = tfidf_matrix[train_indices]\n",
    "test_tfidf_matrix = tfidf_matrix[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Computing Cosine Similarities.\n",
    "Calculate the similarity between each test item and all training items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_test_train = cosine_similarity(test_tfidf_matrix, train_tfidf_matrix)\n",
    "\n",
    "# Convert indices to numpy arrays for indexing\n",
    "train_indices_array = train_indices.to_numpy()\n",
    "test_indices_array = test_indices.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Evaluation Metrics.\n",
    "\n",
    "Evaluate the performance of the recommender system using standard metrics.\n",
    "    Initialize (TP), (FP), (FN), (TN).\n",
    "    Loop through each test item:\n",
    "    Retrieve the test item's categories, skipping items without categories.\n",
    "    Compute similarity scores to all training items.\n",
    "    Select top k recommended items.\n",
    "    For each recommended item, check if it shares categories with the test item.\n",
    "    Update TP and FP counts accordingly.\n",
    "    Calculate FN and TN based on the total number of relevant and non-relevant items.\n",
    "    After processing all test items, compute Precision, Recall, Accuracy, and F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10  # Number of similar contents to recommend\n",
    "\n",
    "# Initialize counters\n",
    "total_TP = 0\n",
    "total_FP = 0\n",
    "total_FN = 0\n",
    "total_TN = 0\n",
    "\n",
    "num_train_items = len(train_indices)\n",
    "num_test_items = len(test_indices)\n",
    "\n",
    "for i, test_idx in enumerate(test_indices_array):\n",
    "    # Get the test item\n",
    "    test_item = cross_content.loc[test_idx]\n",
    "    test_title = test_item['title']\n",
    "    test_categories = set(flatten_categories(test_item['categories_list']))\n",
    "    \n",
    "    # Skip items with no categories\n",
    "    if not test_categories:\n",
    "        continue\n",
    "\n",
    "    # Get the similarity scores to training items\n",
    "    sim_scores = cosine_sim_test_train[i]\n",
    "\n",
    "    # Get the top k indices in training set\n",
    "    top_k_indices_in_train = sim_scores.argsort()[-k:][::-1]\n",
    "    top_k_train_indices = train_indices_array[top_k_indices_in_train]\n",
    "\n",
    "    # Get the recommended items\n",
    "    recommended_items = cross_content.loc[top_k_train_indices]\n",
    "    recommended_titles = recommended_items['title'].tolist()\n",
    "\n",
    "    # Get categories of recommended items\n",
    "    recommended_categories = recommended_items['categories_list'].tolist()\n",
    "\n",
    "    # Flatten and preprocess categories\n",
    "    recommended_categories_flat = [set(flatten_categories(cats)) for cats in recommended_categories]\n",
    "\n",
    "    # Calculate TP and FP\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    for rec_cats in recommended_categories_flat:\n",
    "        if rec_cats & test_categories:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "\n",
    "    # Calculate FN and TN\n",
    "    total_relevant_items = X_cross_train[X_cross_train['categories_list'].apply(lambda x: bool(set(flatten_categories(x)) & test_categories))].shape[0]\n",
    "    total_non_relevant_items = num_train_items - total_relevant_items\n",
    "    FN = total_relevant_items - TP\n",
    "    TN = total_non_relevant_items - FP\n",
    "\n",
    "    total_TP += TP\n",
    "    total_FP += FP\n",
    "    total_FN += FN\n",
    "    total_TN += TN\n",
    "\n",
    "# Compute metrics\n",
    "precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "accuracy = (total_TP + total_TN) / (total_TP + total_FP + total_FN + total_TN) if (total_TP + total_FP + total_FN + total_TN) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Recommendation Function.\n",
    "\n",
    "Try to find the index of the item with the specified title.\n",
    "Compute cosine similarity between the item's TF-IDF vector and all items.\n",
    "Sort the similarity scores in descending order.\n",
    "Exclude the item itself from recommendations.\n",
    "Retrieve the titles of the top k recommended items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get recommendations for a content title\n",
    "def get_recommendations(content_title, k):\n",
    "    # Get the index of the content with the given title\n",
    "    try:\n",
    "        content_index = cross_content[cross_content['title'] == content_title].index[0]\n",
    "    except IndexError:\n",
    "        print(f\"Content titled '{content_title}' not found.\")\n",
    "        return []\n",
    "\n",
    "    # Get the similarity scores for the content_index\n",
    "    # Compute cosine similarity between the content and all contents\n",
    "    sim_scores = cosine_similarity(tfidf_matrix[content_index], tfidf_matrix).flatten()\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    sim_scores_indices = sim_scores.argsort()[::-1]\n",
    "\n",
    "    # Get the indices of the top k+1 similar contents (excluding the content itself)\n",
    "    top_indices = [i for i in sim_scores_indices if i != content_index][:k]\n",
    "\n",
    "    # Get the titles of the recommended contents\n",
    "    recommended_titles = cross_content.iloc[top_indices]['title'].tolist()\n",
    "\n",
    "    return recommended_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Printing Recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content_title = \"Fight Club\"\n",
    "recommendations = get_recommendations(content_title, k)\n",
    "\n",
    "def print_content_type(titles):\n",
    "    for title in titles:\n",
    "        content_type_row = title_type_df[title_type_df['title'] == title]\n",
    "        if not content_type_row.empty:\n",
    "            content_type = content_type_row['content_type'].values[0]\n",
    "            if content_type == 'book':\n",
    "                print(f\"{title} (Book)\")\n",
    "            elif content_type == 'movie':\n",
    "                print(f\"{title} (Movie)\")\n",
    "            else:\n",
    "                print(f\"{title} (Unknown Content Type)\")\n",
    "        else:\n",
    "            print(f\"{title} (Content Type Not Found)\")\n",
    "\n",
    "\n",
    "sorted_recommendations = cross_rating[cross_rating['title'].isin(recommendations)].sort_values(['average_rating', 'ratings_count'], ascending=[False, False])['title'].tolist()\n",
    "print_content_type(sorted_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Printing validation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valuation of the recommender system\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
